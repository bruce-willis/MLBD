{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><a href=\"https://xgboost.readthedocs.io/en/latest/index.html\" target=\"_blank\">XGBoost</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-Model\" data-toc-modified-id=\"Baseline-Model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Baseline Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-Data\" data-toc-modified-id=\"Prepare-Data-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Prepare Data</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Average-Treatment-Effect\" data-toc-modified-id=\"Average-Treatment-Effect-1.1.3.1\"><span class=\"toc-item-num\">1.1.3.1&nbsp;&nbsp;</span>Average Treatment Effect</a></span></li></ul></li></ul></li><li><span><a href=\"#New-Model\" data-toc-modified-id=\"New-Model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>New Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-Data\" data-toc-modified-id=\"Prepare-Data-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Prepare Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Categorical Features</a></span></li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Pipeline</a></span></li></ul></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li></ul></li><li><span><a href=\"#Features-Importance\" data-toc-modified-id=\"Features-Importance-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Features Importance</a></span><ul class=\"toc-item\"><li><span><a href=\"#SHAP\" data-toc-modified-id=\"SHAP-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><a href=\"https://github.com/slundberg/shap\" target=\"_blank\">SHAP</a></a></span></li><li><span><a href=\"#XGBoost-features-importance\" data-toc-modified-id=\"XGBoost-features-importance-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span><a href=\"https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.get_score\" target=\"_blank\">XGBoost features importance</a></a></span></li></ul></li><li><span><a href=\"#Practical-Lessons-From-Facebook\" data-toc-modified-id=\"Practical-Lessons-From-Facebook-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><a href=\"https://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\">Practical Lessons From Facebook</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-Data\" data-toc-modified-id=\"Prepare-Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Prepare Data</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Make-a-Submission\" data-toc-modified-id=\"Make-a-Submission-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Make a Submission</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжаем работать над задачей CTR-prediction с использованием датасета от Criteo.\n",
    "\n",
    "Описание задачи и данных можно посмотреть в notebook'e предыдущей практики (`sgd_logreg_nn/notebooks/ctr_prediction_mllib.ipynb`).\n",
    "\n",
    "# [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "Утановим xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.0.2-py3-none-manylinux1_x86_64.whl (109.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 109.7 MB 4.5 MB/s eta 0:00:01    |███▏                            | 10.9 MB 3.3 MB/s eta 0:00:30\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/lib64/python3.5/site-packages (from xgboost) (1.3.3)\n",
      "Requirement already satisfied: numpy in /usr/lib64/python3.5/site-packages (from xgboost) (1.17.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip3.5 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sys.path.append('./utils')\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\"\n",
    "--jars xgboost4j-spark-0.72.jar,xgboost4j-0.72.jar\n",
    "--py-files sparkxgb.zip pyspark-shell\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"spark_sql_examples\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.task.cpus\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from metrics import rocauc, logloss, ne\n",
    "from processing import split_by_col\n",
    "\n",
    "from sparkxgb.xgboost import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на слудующие строки:\n",
    "\n",
    "* ```python\n",
    "sys.path.append('./utils')\n",
    "...\n",
    "from metrics import rocauc, logloss, ne\n",
    "from processing import split_by_col\n",
    "```\n",
    "\n",
    "В папке `utils` находится два файла (`metrics.py`, `processing.py`), которые содержат функции, которые нужно было реализовать в рамках предыдущей практики.\n",
    "\n",
    "\n",
    "* ```python\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\"\n",
    "--jars xgboost4j-spark-0.72.jar,xgboost4j-0.72.jar\n",
    "--py-files sparkxgb.zip pyspark-shell\n",
    "\"\"\"\n",
    "...\n",
    "from sparkxgb.xgboost import *\n",
    "```\n",
    "\n",
    "Для того чтобы в рамках инфраструктуры Spark можно было использовать XGBoost, мы воспользуемся библиотекой [XGBoost4J](https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html).\n",
    "\n",
    "В ходе выполнения занятий может быть полезно ознакомиться с исходным кодом обертки для питона, который находится в архиве `sparkxgb.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/workspace/data/criteo'\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = OrderedDict() # in python 3.6 will work with just {} because dict ordered in insertion order by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(False, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns = ['_c{}'.format(i) for i in range(1, 14)]\n",
    "cat_columns = ['_c{}'.format(i) for i in range(14, 40)][:2]\n",
    "len(num_columns), len(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся пайплайном из предыдущей практики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "pipeline_model = PipelineModel.load(os.path.join(DATA_PATH, 'pipeline_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_c42773d6ff43,\n",
       " StringIndexer_25efb5034eb3,\n",
       " OneHotEncoderEstimator_99443336b5be,\n",
       " VectorAssembler_32b1969e093c]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1396, 545)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipeline_model.stages[0].labels), len(pipeline_model.stages[1].labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая размерность пространства фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1954"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(num_columns) + len(pipeline_model.stages[0].labels) + len(pipeline_model.stages[1].labels)\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1831957"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pipeline_model \\\n",
    "    .transform(df) \\\n",
    "    .select(F.col('_c0').alias('label'), 'features', 'id') \\\n",
    "    .cache()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_by_col(df, 'id', [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBoostEstimator(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    colsample_bytree=0.9,\n",
    "    eta=0.15,\n",
    "    gamma=0.9,\n",
    "    max_depth=8,\n",
    "    min_child_weight=50.0,\n",
    "    subsample=0.9,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss', \n",
    "    silent=0,\n",
    "    num_round=20,\n",
    "    nthread=2,\n",
    "    nworkers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 ms, sys: 13.7 ms, total: 58.7 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = estimator.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем [booster](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster) обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._call_java(\"booster\").saveModel(os.path.join(DATA_PATH, 'xgb.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Treatment Effect\n",
    "\n",
    "Пусть даны две экспериментальные группы treatment ($T$) и control ($C$), где\n",
    "\n",
    "* `treatment` - группа с изменением (например, новая модель)\n",
    "* `control` - группа без изменений\n",
    "\n",
    "Рассмотрим метрику $X$, значение которой мы расчитали для наших групп ($X_T, X_C$).\n",
    "\n",
    "Тогда под ATE будем иметь в виду\n",
    "$$ \\Delta\\% = \\frac{X_T - X_C}{X_C} \\cdot 100 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ate(groups, control_name) -> pd.DataFrame:\n",
    "    \"\"\"Get Average Treatment Effect\n",
    "    groups - dictionary where keys - names of models, values - dicts of pairs <metric_name>, <metric_value>\n",
    "    control_name - name of baseline model\n",
    "    \n",
    "    return pd.DataFrame (rows corresponds to metrics, cols corresponds to models and ATE with respect to control)\n",
    "    \"\"\"\n",
    "    metrics = pd.DataFrame.from_records(groups, columns=groups.keys())\n",
    "    return metrics.subtract(metrics['xgb_baseline'], axis='index') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7293977450019278"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(model, val_df, probabilities_col='probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7281438220858361"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_metrics['ROC AUC'] = rocauc(model, test_df, probabilities_col='probabilities')\n",
    "baseline_metrics['ROC AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['xgb_baseline'] = baseline_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним результаты с логрег моделью из предыдущей практики.\n",
    "\n",
    "1. Загрузить обученную `LogReg` модель\n",
    "2. Посчитать метрики на `test_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "logreg_path = os.path.join(DATA_PATH, 'logreg_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_9229974147cc, numClasses = 2, numFeatures = 1954"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegressionModel.load(logreg_path)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "logrec_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6999639315838841"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrec_metrics['ROC AUC'] = rocauc(lr_model, test_df, probabilities_col='probability')\n",
    "logrec_metrics['ROC AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['logreg'] = logrec_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить таблицу ATE используя метод `get_ate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb_baseline</th>\n",
       "      <th>logreg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.817989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xgb_baseline    logreg\n",
       "ROC AUC           0.0 -2.817989"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ate(all_metrics, 'xgb_baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TRAIN_PATH)\n",
    "\n",
    "df = df.sample(False, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features\n",
    "\n",
    "See [Doc](https://spark.apache.org/docs/latest/ml-pipeline.html) for additional details on Transformers and Encoders.\n",
    "\n",
    "Implement classes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted Model\n",
    "class MeanTargetEncoderModel(pyspark.ml.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Estimator\n",
    "class MeanTargetEncoder(pyspark.ml.Estimator):\n",
    "    \n",
    "    def __init__(self, inputCol, targetCol, featuresCol):\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "cat_enc_columns = [cat_col + '_enc' for cat_col in cat_columns]\n",
    "\n",
    "mean_target_encoders = [MeanTargetEncoder(cat_col, '_c0', cat_enc_col) \n",
    "                        for cat_col, cat_enc_col in zip(cat_columns, cat_enc_columns)]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=num_columns + cat_enc_columns, outputCol=\"features\").setHandleInvalid(\"keep\")\n",
    "\n",
    "pipeline = Pipeline(stages=mean_target_encoders + [assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(df)\n",
    "pipeline_model.save(os.path.join(DATA_PATH, 'pipeline_model_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "pipeline_model = PipelineModel.load(os.path.join(DATA_PATH, 'pipeline_model_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline_model \\\n",
    "    .transform(df) \\\n",
    "    .select(F.col('_c0').alias('label'), 'features', 'id') \\\n",
    "    .cache()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Train XGBoost on the new set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнить результаты новой модели с `xgb_baseline` и `log_reg` с помощью функции `get_ate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Importance\n",
    "\n",
    "## [SHAP](https://github.com/slundberg/shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.5 install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "\n",
    "bst = xgb.Booster()\n",
    "bst.load_model(os.path.join(DATA_PATH, 'xgb.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имея `booster` модели можно, например, посмотреть на то какие деревья получились в итоге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bst.get_dump()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(False, 0.05)\n",
    "sample_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "def df_to_csr(df, dim):\n",
    "    data = []\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    \n",
    "    sparse_vecs = df.rdd.map(lambda row: row.features).collect()\n",
    "    for i, vec in enumerate(sparse_vecs):\n",
    "        for idx, val in zip(vec.indices, vec.values):\n",
    "            data.append(val)\n",
    "            row_ind.append(i)\n",
    "            col_ind.append(idx)\n",
    "        \n",
    "    return csr_matrix((data, (row_ind, col_ind)), shape=(len(sparse_vecs), dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = df_to_csr(sample_df, dim)\n",
    "dtest = xgb.DMatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arr = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "explainer = shap.TreeExplainer(bst)\n",
    "shap_values = explainer.shap_values(dtest, tree_limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_arr, max_display=20, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [XGBoost features importance](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.get_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_score(booster, importance):\n",
    "    gains_xgb = booster.get_score(importance_type=importance)\n",
    "    gains = {}\n",
    "    for f, g in gains_xgb.items():\n",
    "        gains[f] = g\n",
    "    sorted_gains = sorted(list(gains.items()), key=lambda x: -x[1])\n",
    "    return sorted_gains\n",
    "\n",
    "\n",
    "features_scores = get_feature_score(bst, 'gain')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "f_names, f_scores = zip(*features_scores)\n",
    "features_scores_pdf = pd.DataFrame({'feature': f_names, 'gain': f_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8,8))\n",
    "ax = sns.barplot(x='gain', y='feature', data=features_scores_pdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Practical Lessons From Facebook](https://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n",
    "\n",
    "## Prepare Data\n",
    "\n",
    "* Реализуйте модель из статьи (LogReg поверх XGBoost)\n",
    "\n",
    "* Попробуйте реализовать Negatives Subsampling + Re-calibration описанный в статье (доп. баллы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(model, df):\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните новую модель со всеми предыдущими с помощью `get_ate`. При сравнении использовать еще и метрику calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission\n",
    "\n",
    "Если в результате работы получилась модель, которая лучше чем ЛогРег из предыдущей практики, то точно нужно сделать submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
